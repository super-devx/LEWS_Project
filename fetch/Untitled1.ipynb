{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.2.0\n",
      "Eager execution: True\n",
      "Local copy of the dataset file: /home/server1/.keras/datasets/iris_training.csv\n",
      "Features: ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
      "Label: species\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"TensorFlow version: {}\".format(tf.__version__))\n",
    "print(\"Eager execution: {}\".format(tf.executing_eagerly()))\n",
    "\n",
    "train_dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv\"\n",
    "\n",
    "train_dataset_fp = tf.keras.utils.get_file(fname=os.path.basename(train_dataset_url),origin=train_dataset_url)\n",
    "\n",
    "print(\"Local copy of the dataset file: {}\".format(train_dataset_fp))\n",
    "\n",
    "\n",
    "column_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n",
    "\n",
    "feature_names = column_names[:-1]\n",
    "label_name = column_names[-1]\n",
    "\n",
    "print(\"Features: {}\".format(feature_names))\n",
    "print(\"Label: {}\".format(label_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('sepal_length', <tf.Tensor: shape=(32,), dtype=float32, numpy=\n",
      "array([5.4, 6.4, 6.4, 5.7, 6. , 5.1, 6.3, 5.2, 6.4, 6. , 5. , 5.8, 5.3,\n",
      "       4.8, 4.5, 6.1, 4.8, 5.5, 5.4, 5. , 6.7, 5.7, 6.3, 4.4, 5.9, 5.2,\n",
      "       4.4, 5. , 5.1, 7.9, 4.9, 4.9], dtype=float32)>), ('sepal_width', <tf.Tensor: shape=(32,), dtype=float32, numpy=\n",
      "array([3.9, 2.8, 3.1, 3.8, 3. , 3.8, 3.3, 2.7, 2.8, 2.2, 3.4, 2.8, 3.7,\n",
      "       3.4, 2.3, 3. , 3. , 2.6, 3. , 3. , 3.1, 3. , 3.3, 3. , 3.2, 3.4,\n",
      "       3.2, 3.5, 2.5, 3.8, 3.1, 3.1], dtype=float32)>), ('petal_length', <tf.Tensor: shape=(32,), dtype=float32, numpy=\n",
      "array([1.3, 5.6, 5.5, 1.7, 4.8, 1.6, 4.7, 3.9, 5.6, 5. , 1.6, 5.1, 1.5,\n",
      "       1.6, 1.3, 4.9, 1.4, 4.4, 4.5, 1.6, 4.4, 4.2, 6. , 1.3, 4.8, 1.4,\n",
      "       1.3, 1.3, 3. , 6.4, 1.5, 1.5], dtype=float32)>), ('petal_width', <tf.Tensor: shape=(32,), dtype=float32, numpy=\n",
      "array([0.4, 2.2, 1.8, 0.3, 1.8, 0.2, 1.6, 1.4, 2.1, 1.5, 0.4, 2.4, 0.2,\n",
      "       0.2, 0.3, 1.8, 0.3, 1.2, 1.5, 0.2, 1.4, 1.2, 2.5, 0.2, 1.8, 0.2,\n",
      "       0.2, 0.3, 1.1, 2. , 0.1, 0.1], dtype=float32)>)])\n",
      "tf.Tensor([0 2 2 0 2 0 1 1 2 2 0 2 0 0 0 2 0 1 1 0 1 1 2 0 1 0 0 0 1 2 0 0], shape=(32,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataset = tf.data.experimental.make_csv_dataset(train_dataset_fp,batch_size,column_names=column_names,\n",
    "    label_name=label_name,num_epochs=1)\n",
    "\n",
    "features, labels = next(iter(train_dataset))\n",
    "\n",
    "print(features)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[6.5 3.  5.5 1.8]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.  3.  1.6 0.2]], shape=(32, 4), dtype=float32)\n",
      "tf.Tensor([2 0 1 0 1 2 1 0 0 0 2 0 0 2 0 2 2 0 0 0 2 0 1 0 0 0 1 2 0 0 1 0], shape=(32,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "def pack_features_vector(features, labels):\n",
    "  \"\"\"Pack the features into a single array.\"\"\"\n",
    "  features = tf.stack(list(features.values()), axis=1)\n",
    "  return features, labels\n",
    "\n",
    "features, labels=pack_features_vector(features, labels)\n",
    "print(features)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(pack_features_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss test: 2.3085553646087646\n",
      "Step: 0, Initial Loss: 2.3085553646087646\n",
      "Step: 1,         Loss: 1.646769404411316\n",
      "Epoch 000: Loss: 1.311, Accuracy: 30.000%\n",
      "Epoch 050: Loss: 0.293, Accuracy: 95.000%\n",
      "Epoch 100: Loss: 0.214, Accuracy: 97.500%\n",
      "Epoch 150: Loss: 0.166, Accuracy: 97.500%\n",
      "Epoch 200: Loss: 0.139, Accuracy: 97.500%\n",
      "Epoch 250: Loss: 0.119, Accuracy: 97.500%\n",
      "Epoch 300: Loss: 0.101, Accuracy: 97.500%\n",
      "Epoch 350: Loss: 0.100, Accuracy: 97.500%\n",
      "Epoch 400: Loss: 0.094, Accuracy: 97.500%\n",
      "Epoch 450: Loss: 0.103, Accuracy: 97.500%\n",
      "Epoch 500: Loss: 0.097, Accuracy: 98.333%\n",
      "Epoch 550: Loss: 0.080, Accuracy: 99.167%\n",
      "Epoch 600: Loss: 0.082, Accuracy: 97.500%\n",
      "Epoch 650: Loss: 0.087, Accuracy: 97.500%\n",
      "Epoch 700: Loss: 0.074, Accuracy: 98.333%\n",
      "Epoch 750: Loss: 0.081, Accuracy: 98.333%\n",
      "Epoch 800: Loss: 0.110, Accuracy: 98.333%\n",
      "Epoch 850: Loss: 0.064, Accuracy: 99.167%\n",
      "Epoch 900: Loss: 0.067, Accuracy: 96.667%\n",
      "Epoch 950: Loss: 0.074, Accuracy: 97.500%\n",
      "Epoch 1000: Loss: 0.078, Accuracy: 98.333%\n",
      "Epoch 1050: Loss: 0.069, Accuracy: 99.167%\n",
      "Epoch 1100: Loss: 0.063, Accuracy: 99.167%\n",
      "Epoch 1150: Loss: 0.069, Accuracy: 97.500%\n",
      "Epoch 1200: Loss: 0.081, Accuracy: 97.500%\n",
      "Epoch 1250: Loss: 0.062, Accuracy: 97.500%\n",
      "Epoch 1300: Loss: 0.060, Accuracy: 98.333%\n",
      "Epoch 1350: Loss: 0.058, Accuracy: 98.333%\n",
      "Epoch 1400: Loss: 0.062, Accuracy: 98.333%\n",
      "Epoch 1450: Loss: 0.062, Accuracy: 99.167%\n",
      "Epoch 1500: Loss: 0.057, Accuracy: 98.333%\n",
      "Epoch 1550: Loss: 0.071, Accuracy: 98.333%\n",
      "Epoch 1600: Loss: 0.070, Accuracy: 98.333%\n",
      "Epoch 1650: Loss: 0.053, Accuracy: 98.333%\n",
      "Epoch 1700: Loss: 0.082, Accuracy: 98.333%\n",
      "Epoch 1750: Loss: 0.061, Accuracy: 97.500%\n",
      "Epoch 1800: Loss: 0.064, Accuracy: 99.167%\n",
      "Epoch 1850: Loss: 0.064, Accuracy: 97.500%\n",
      "Epoch 1900: Loss: 0.076, Accuracy: 98.333%\n",
      "Epoch 1950: Loss: 0.051, Accuracy: 98.333%\n",
      "Epoch 2000: Loss: 0.060, Accuracy: 97.500%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = tf.keras.Sequential([tf.keras.layers.Dense(10, activation=tf.nn.relu, input_shape=(4,)),  # input shape required\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.relu),tf.keras.layers.Dense(3)])\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "def loss(model, x, y, training):\n",
    "  # training=training is needed only if there are layers with different\n",
    "  # behavior during training versus inference (e.g. Dropout).\n",
    "  y_ = model(x, training=training)\n",
    "\n",
    "  return loss_object(y_true=y, y_pred=y_)\n",
    "\n",
    "\n",
    "l = loss(model, features, labels, training=False)\n",
    "print(\"Loss test: {}\".format(l))\n",
    "\n",
    "\n",
    "def grad(model, inputs, targets):\n",
    "  with tf.GradientTape() as tape:\n",
    "    loss_value = loss(model, inputs, targets, training=True)\n",
    "  return loss_value, tape.gradient(loss_value, model.trainable_variables)\n",
    "\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "loss_value, grads = grad(model, features, labels)\n",
    "\n",
    "print(\"Step: {}, Initial Loss: {}\".format(optimizer.iterations.numpy(),\n",
    "                                          loss_value.numpy()))\n",
    "\n",
    "optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "print(\"Step: {},         Loss: {}\".format(optimizer.iterations.numpy(),\n",
    "                                          loss(model, features, labels, training=True).numpy()))\n",
    "\n",
    "\n",
    "## Note: Rerunning this cell uses the same model variables\n",
    "\n",
    "# Keep results for plotting\n",
    "train_loss_results = []\n",
    "train_accuracy_results = []\n",
    "\n",
    "num_epochs = 2001\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "  epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "  # Training loop - using batches of 32\n",
    "  for x, y in train_dataset:\n",
    "    # Optimize the model\n",
    "    loss_value, grads = grad(model, x, y)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    # Track progress\n",
    "    epoch_loss_avg.update_state(loss_value)  # Add current batch loss\n",
    "    # Compare predicted label to actual label\n",
    "    # training=True is needed only if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    epoch_accuracy.update_state(y, model(x, training=True))\n",
    "\n",
    "  # End epoch\n",
    "  train_loss_results.append(epoch_loss_avg.result())\n",
    "  train_accuracy_results.append(epoch_accuracy.result())\n",
    "\n",
    "  if epoch % 50 == 0:\n",
    "    print(\"Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}\".format(epoch,\n",
    "                                                                epoch_loss_avg.result(),\n",
    "                                                                epoch_accuracy.result()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 96.667%\n"
     ]
    }
   ],
   "source": [
    "test_url = \"https://storage.googleapis.com/download.tensorflow.org/data/iris_test.csv\"\n",
    "\n",
    "test_fp = tf.keras.utils.get_file(fname=os.path.basename(test_url),\n",
    "                                  origin=test_url)\n",
    "\n",
    "\n",
    "test_dataset = tf.data.experimental.make_csv_dataset(\n",
    "    test_fp,\n",
    "    batch_size,\n",
    "    column_names=column_names,\n",
    "    label_name='species',\n",
    "    num_epochs=1,\n",
    "    shuffle=False)\n",
    "\n",
    "test_dataset = test_dataset.map(pack_features_vector)\n",
    "\n",
    "test_accuracy = tf.keras.metrics.Accuracy()\n",
    "\n",
    "for (x, y) in test_dataset:\n",
    "  # training=False is needed only if there are layers with different\n",
    "  # behavior during training versus inference (e.g. Dropout).\n",
    "  logits = model(x, training=False)\n",
    "  prediction = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
    "  test_accuracy(prediction, y)\n",
    "\n",
    "print(\"Test set accuracy: {:.3%}\".format(test_accuracy.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0 prediction: Iris setosa (99.9%)\n",
      "Example 1 prediction: Iris versicolor (99.9%)\n",
      "Example 2 prediction: Iris virginica (94.2%)\n"
     ]
    }
   ],
   "source": [
    "class_names = ['Iris setosa', 'Iris versicolor', 'Iris virginica']\n",
    "predict_dataset = tf.convert_to_tensor([\n",
    "    [5.1, 3.3, 1.7, 0.5,],\n",
    "    [5.9, 3.0, 4.2, 1.5,],\n",
    "    [6.9, 3.1, 5.4, 2.1]\n",
    "])\n",
    "\n",
    "# training=False is needed only if there are layers with different\n",
    "# behavior during training versus inference (e.g. Dropout).\n",
    "predictions = model(predict_dataset, training=False)\n",
    "\n",
    "for i, logits in enumerate(predictions):\n",
    "  class_idx = tf.argmax(logits).numpy()\n",
    "  p = tf.nn.softmax(logits)[class_idx]\n",
    "  name = class_names[class_idx]\n",
    "  print(\"Example {} prediction: {} ({:4.1f}%)\".format(i, name, 100*p))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
